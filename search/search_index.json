{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Transport for London Cycling Data","text":"<p>Exploratory Data Analysis of the Transport for London (TfL) Cycling trip data.</p>"},{"location":"#overview","title":"Overview","text":"<p>The main goal is to apply the knowledge acquired during the Google Data Analytics Professional Certificate course, while keeping it simple enough to fit in the plaintext format of a github repository.</p> <p>I will analyze a single year worth of data, to limit the scope of the analysis.</p> <p>I will work with the data from 2024 since is the most recent complete year at the time of writing.</p>"},{"location":"#tools","title":"Tools","text":"<p>I will be working mainly with:</p> Tool Use DuckDB Parse, clean and query the dataset Mermaid General purpose diagrams Seaborn Complex plots"},{"location":"#sections","title":"Sections","text":"<ul> <li>Prepare: Getting the dataset ready for exploration.</li> <li>Explore: Exploratory Data Analysis of main entities and attributes.</li> <li>Process: Clean and transform the original dataset.</li> <li>Analyze</li> </ul>"},{"location":"analyze/","title":"Analyze","text":"<p>Now we will make a review of the main entities of the dataset:</p> <ul> <li>Trips</li> <li>Bikes</li> <li>Stations</li> <li>Routes</li> </ul> <p>We will use simple statistical visualizations to gain knowledge on the attributes value ranges, distributions, correlations and general quirks before diving into modeling the relationships with more complex analysis tools.</p>"},{"location":"analyze/bikes/","title":"Bikes","text":"<p>A total of 14920 unique bikes were used during 2024.</p> <pre><code>SELECT COUNT(DISTINCT bike_id) FROM trips;\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 count(DISTINCT bike_id) \u2502\n\u2502          int64          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502          14920          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"analyze/bikes/#bike-trip-count","title":"Bike trip count","text":"<p>The trip count among all bikes shows a multi-mode distribution.</p> <p></p> <p>Possible causes:</p> <ul> <li>Location: Some bikes move between high-traffic stations.</li> <li>Age: New bikes could be added in batches mid-year.</li> </ul>"},{"location":"analyze/bikes/#bike-first-use-date","title":"Bike first use date","text":"<p>We get the first use date for each bike and plotting the cumulative proportion of bikes used as the year goes on.</p> <p></p> <p>We assume that a bike's first use is a good indicator of the bikes age inside the system. The bike could be new or was assigned a new ID after refurbishing.</p> <p>Here we can see that:</p> <ul> <li>most of the bikes were used in the first 2 months of the year.</li> <li>a steady amount of new bikes enters the float all-year round.</li> <li>a big increase in bikes is observed around July and October.</li> </ul>"},{"location":"analyze/bikes/#bike-age-vs-trip-count","title":"Bike age vs Trip count","text":"<p>Checking the correlation between a bike's first trip date and the total trip count we observe:</p> <ul> <li>Bike Age and Trip count have an inverse relationship.</li> <li>Older bikes have a wide distribution of trip counts.</li> <li>Both observed groups of new bikes (July and October) have similar trip counts and represent 2 of the modes in the trip count distribution.</li> </ul> <p></p>"},{"location":"analyze/trips/","title":"Trips","text":""},{"location":"analyze/trips/#trip-duration","title":"Trip duration","text":"<p>Trip duration times range from a few seconds to many days. With most them falling under around 120 minutes.</p> <p></p> <p></p>"},{"location":"analyze/trips/#round-trips","title":"Round-trips","text":"<p>Flagging trips with the same start and end station as round trips reveals some insights.</p> <p>Round trips are much less common than one-way trips.</p> <pre><code>SELECT\n  (station_end_id == station_start_id) AS round_trip,\n  COUNT(*) as count\nFROM trips_raw\nGROUP BY round_trip;\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 round_trip \u2502  count  \u2502\n\u2502  boolean   \u2502  int64  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 false      \u2502 8434689 \u2502\n\u2502 true       \u2502  320463 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p></p> <p>Closer inspection of round trips reveals a double mode.</p> <p></p> <p>Some possible causes:</p> <ul> <li>short trips:<ul> <li>users changing their mind about using the bike</li> <li>users testing the app functionality and/or the bikes themselves</li> <li>users taking short trips around the block</li> </ul> </li> <li>average trips:<ul> <li>users running errands</li> <li>taking longer pleasure routes</li> </ul> </li> </ul>"},{"location":"analyze/trips/#trips-under-1-minute","title":"Trips under 1 minute","text":"<p>There is an abundance of trips under 1 minute.</p> <p></p> <pre><code>SELECT\n  (station_end_id == station_start_id) AS round_trip,\n  COUNT(*) as count\nFROM trips_raw\nWHERE duration_ms / (1000 * 60) &lt; 1\nGROUP BY round_trip;\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 round_trip \u2502 count \u2502\n\u2502  boolean   \u2502 int64 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 false      \u2502  2148 \u2502\n\u2502 true       \u2502 67117 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Removing the round trips of less than 1 minute we get a more natural distribution.</p> <p></p> <p>Without more information it's impossible to discern the quality of the remaining 2 thousand trips records. Some possibilities:</p> <ul> <li>The two stations are really close and make it possible to get between them in less than a minute.</li> <li>Data quality issue, assigning the wrong ID to the end station.</li> </ul> <p>Going forward I will only focus on trips with durations:</p> <ul> <li>less or equal to 60 minutes</li> <li>different start and end locations if <code>duration_minutes</code> is less than 1 minute.</li> </ul> <pre><code>DELETE FROM trips\nWHERE\nduration_minutes &gt; 60\nOR\n(duration_minutes &lt; 1 AND station_start_id == station_end_id);\n</code></pre>"},{"location":"explore/","title":"Explore","text":"<p>In this section we perform Exploratory Data Analysis (EDA) to find the distribution of the dataset attributes, generate investigation hypothesis and find inconsistencies.</p>"},{"location":"explore/#contents","title":"Contents","text":"<ul> <li>Dates</li> <li>Bikes</li> <li>Stations</li> <li>Trips Duration</li> </ul>"},{"location":"explore/bikes/","title":"Bikes","text":""},{"location":"explore/bikes/#bike-ids","title":"Bike IDs","text":"<p>We verify that there are no <code>NULL</code> <code>bike_id</code>.</p> <pre><code>SELECT COUNT(bike_id) as bike_id_null_count\nFROM trips_raw\nWHERE bike_id IS NULL;\n</code></pre> bike_id_null_count 0 <p>We check the extremes of the <code>bike_id</code> attribute to find weird records.</p> <pre><code>SELECT DISTINCT bike_id\nFROM trips_raw\nORDER BY bike_id ASC\nLIMIT 10;\n</code></pre> bike_id 2 4 6 9 10168 10169 10274 10300 10307 10310 <pre><code>SELECT DISTINCT bike_id\nFROM trips_raw\nORDER BY bike_id DESC\nLIMIT 10;\n</code></pre> bike_id 99997 99993 99984 99982 99975 63550 63549 63548 63547 63546 <p>We identify Bikes 2, 4, 6, 9, 99975, 99982, 99984, 99993 and 99997 as suspicious, so we proceed to gather some general information and compare it against the other bikes in the dataset.</p> <pre><code>SELECT\n  bike_id,\n  COUNT() AS trip_count,\n  strftime(MIN(date_start),'%Y-%m-%d') AS first_seen,\n  strftime(MAX(date_end),'%Y-%m-%d') AS  last_seen,\nFROM trips_raw\nWHERE bike_id NOT IN [2,4,6,9,99997,99993,99984,99982,99975]\nGROUP BY bike_id\nORDER BY trip_count DESC\nLIMIT 20;\n</code></pre> bike_id trip_count first_seen last_seen 2 553 2024-01-11 2024-12-20 4 579 2024-01-02 2024-12-31 6 186 2024-07-20 2024-12-31 9 449 2024-05-25 2024-12-24 99975 1 2024-10-06 2024-10-06 99982 1 2024-02-08 2024-02-08 99984 3 2024-05-13 2024-12-08 99993 1 2024-12-30 2024-12-30 99997 1 2024-08-19 2024-08-19 <p>We gather the same information about the least and most used bikes among all the non-suspicious ones.</p> <pre><code>SELECT\n  bike_id,\n  COUNT() AS trip_count,\n  strftime(MIN(date_start),'%Y-%m-%d') AS first_seen,\n  strftime(MAX(date_end),'%Y-%m-%d') AS  last_seen,\nFROM trips_raw\nWHERE bike_id NOT IN [2, 4, 6, 9, 99975, 99982, 99984, 99993, 99997]\nGROUP BY bike_id\nORDER BY trip_count DESC, bike_id\nLIMIT 10;\n</code></pre> bike_id trip_count first_seen last_seen 56472 1403 2024-01-01 2024-12-25 58857 1402 2024-01-01 2024-12-30 58920 1403 2024-01-02 2024-12-31 59410 1430 2024-01-01 2024-12-31 59455 1420 2024-01-03 2024-12-30 59610 1408 2024-01-09 2024-12-28 59633 1423 2024-01-05 2025-01-01 59688 1430 2024-01-03 2024-12-29 59751 1463 2024-01-16 2024-12-31 59882 1417 2024-01-01 2024-12-31 <pre><code>SELECT\n  bike_id,\n  COUNT() AS trip_count,\n  strftime(MIN(date_start),'%Y-%m-%d') AS first_seen,\n  strftime(MAX(date_end),'%Y-%m-%d') AS  last_seen,\nFROM trips_raw\nWHERE bike_id NOT IN [2,4,6,9,99997,99993,99984,99982,99975]\nGROUP BY bike_id\nORDER BY trip_count, bike_id\nLIMIT 10;\n</code></pre> bike_id trip_count first_seen last_seen 12835 1 2024-01-01 2024-01-01 21823 1 2024-01-02 2024-01-02 22232 1 2024-12-31 2024-12-31 22438 1 2024-01-05 2024-01-05 22734 1 2024-12-31 2024-12-31 23085 1 2024-06-11 2024-06-12 51141 1 2024-03-13 2024-03-13 60486 1 2024-01-01 2024-01-01 50166 2 2024-09-10 2024-10-08 53154 2 2024-01-03 2024-01-03 <p>We decided that the suspicious bikes don't have any noticeable difference in behavior whe comparing against the rest of the bikes in the dataset.</p>"},{"location":"explore/bikes/#low-trip-count-bikes","title":"Low trip count bikes","text":"<p>After identifying several bikes with high ID and low trip count, we decided to give a closer look to all bikes with low trip count. However, we did not find any suspicious patterns on their trip duration, stations, date or time. So we decided to not explore further in that direction.</p> <pre><code>WITH bids AS (\n  SELECT bike_id\n  FROM trips_raw\n  GROUP BY bike_id\n  HAVING COUNT() &lt;= 3\n)\nSELECT\ntrip_id,\nbike_id,\nbike_model,\ndate_start,\ndate_end,\nstation_start_id,\nstation_end_id,\nduration_text\nFROM trips_raw\nWHERE bike_id IN (SELECT bike_id FROM bids)\nORDER BY date_start;\n</code></pre> trip_id bike_id bike_model date_start date_end station_start_id station_end_id duration_text 136450452 12835 CLASSIC 2024-01-01 00:20:00 2024-01-01 00:44:00 200039 991 23m 57s 136461635 60486 PBSC_EBIKE 2024-01-01 14:30:00 2024-01-01 14:32:00 1195 1195 1m 30s 136467442 21823 CLASSIC 2024-01-02 19:53:00 2024-01-02 20:03:00 200133 200240 10m 10s 136472765 53154 CLASSIC 2024-01-03 08:55:00 2024-01-03 09:07:00 1104 1044 11m 24s 136479867 53154 CLASSIC 2024-01-03 17:31:00 2024-01-03 17:44:00 1049 1100 13m 8s 136499834 22438 CLASSIC 2024-01-05 08:37:00 2024-01-05 08:55:00 1115 200096 17m 47s 137177798 99982 CLASSIC 2024-02-08 10:25:00 2024-02-08 10:28:00 1221 1034 3m 25s 137840860 51141 CLASSIC 2024-03-13 07:45:00 2024-03-13 08:03:00 300031 1164 18m 22s 139296798 99984 CLASSIC 2024-05-13 09:04:00 2024-05-13 09:04:00 2692 2692 26s 140101668 23085 CLASSIC 2024-06-11 12:48:00 2024-06-12 10:46:00 22181 22181 21h 58m 19s 140952329 62812 PBSC_EBIKE 2024-07-10 17:20:00 2024-07-10 17:34:00 10624 1087 13m 29s 140953497 62812 PBSC_EBIKE 2024-07-10 17:38:00 2024-07-10 17:53:00 1087 1162 14m 10s 142176513 99997 CLASSIC 2024-08-19 13:37:00 2024-08-19 13:43:00 1027 3423 5m 46s 142771136 50166 CLASSIC 2024-09-10 14:45:00 2024-09-10 14:55:00 300211 1079 9m 41s 143477154 99975 CLASSIC 2024-10-06 14:08:00 2024-10-06 14:14:00 2682 1041 6m 12s 143524599 50166 CLASSIC 2024-10-08 10:56:00 2024-10-08 11:01:00 300081 300024 5m 23s 143576758 99984 CLASSIC 2024-10-10 08:17:00 2024-10-10 08:31:00 300069 1107 14m 38s 145040410 99984 CLASSIC 2024-12-06 14:06:00 2024-12-08 09:07:00 1200 3469 1d 19h 0m 30s 145309013 63496 PBSC_EBIKE 2024-12-20 07:33:00 2024-12-20 07:55:00 200023 300237 21m 14s 145313862 63496 PBSC_EBIKE 2024-12-20 11:59:00 2024-12-20 12:16:00 300237 2683 17m 29s 145315365 63496 PBSC_EBIKE 2024-12-20 13:18:00 2024-12-20 13:38:00 2683 22171 20m 4s 145330628 30114 CLASSIC 2024-12-21 15:52:00 2024-12-21 16:12:00 2636 2633 19m 52s 145334391 30114 CLASSIC 2024-12-21 21:24:00 2024-12-21 21:29:00 2633 1143 4m 56s 145362698 41547 CLASSIC 2024-12-24 16:26:00 2024-12-24 16:29:00 300070 300075 3m 6s 145365262 30114 CLASSIC 2024-12-24 23:08:00 2024-12-24 23:16:00 1143 300088 8m 23s 145380843 41547 CLASSIC 2024-12-25 16:53:00 2024-12-25 17:26:00 300075 300075 33m 2s 145425814 41547 CLASSIC 2024-12-30 09:12:00 2024-12-30 09:51:00 300075 200212 38m 14s 145434689 99993 CLASSIC 2024-12-30 19:02:00 2024-12-30 19:04:00 300224 300224 1m 30s 145445352 22734 CLASSIC 2024-12-31 16:23:00 2024-12-31 16:34:00 2694 200183 11m 46s 145448676 22232 CLASSIC 2024-12-31 21:37:00 2024-12-31 21:50:00 300207 10628 13m 9s"},{"location":"explore/dates/","title":"Dates","text":""},{"location":"explore/dates/#null-start-or-end-date","title":"NULL Start or End Date","text":"<p>We check that no records have <code>NULL</code> <code>date_start</code> o <code>date_end</code>.</p> <pre><code>SELECT COUNT(*) as null_date_count\nFROM trips_raw\nWHERE date_start IS NULL\nOR date_end IS NULL;\n</code></pre> null_date_count 0"},{"location":"explore/dates/#date-ranges","title":"Date ranges","text":"<pre><code>WITH trip_dates AS(\n    SELECT date_start AS trip_date FROM trips_raw\n    UNION\n    SELECT date_end AS trip_date FROM trips_raw\n)\nSELECT MIN(trip_date), MAX(trip_date)\nFROM trip_dates;\n</code></pre> min(trip_date) max(trip_date) 2024-01-01 00:01:00 2025-02-02 14:38:00 <p>We find that no trip record started on 2023 and that some trips ended on 2025, so we verify that all those trips effectively started on 2024.</p> <pre><code>SELECT\n    YEAR(date_start) as start_year,\n    COUNT() as trip_count\nFROM trips_raw\nWHERE YEAR(date_end) = 2025\nGROUP BY start_year;\n</code></pre> start_year trip_count 2024 328 <p>They do, so we consider all those records as part of 2024.</p>"},{"location":"explore/stations/","title":"Stations","text":""},{"location":"explore/stations/#_old-stations","title":"_OLD Stations","text":"<p>While exploring the dataset, we found many stations with a name suffixed with <code>_OLD</code>.</p> <pre><code>WITH\nstations AS (\n  SELECT\n    station_start_id AS station_id,\n    station_start_name AS station_name,\n  FROM trips_raw\n  UNION ALL\n  SELECT\n    station_end_id AS station_id,\n    station_end_name AS station_name,\n  FROM trips_raw\n)\nSELECT\n  station_id,\n  station_name\nFROM stations\nWHERE contains(station_name, '_OLD')\nGROUP BY station_id, station_name\nORDER BY station_id;\n</code></pre> station_id station_name 300100 Limburg Road, Clapham Junction_OLD 1020444 Leonard Circus , Shoreditch_OLD 1059444 Albert Embankment, Vauxhall_OLD 1190444 Kennington Lane Rail Bridge, Vauxhall_OLD 3429444 Abbey Orchard Street, Westminster_OLD 200032444 Kennington Oval, Oval_OLD 200083444 Pritchard's Road, Bethnal Green_OLD <p>Further exploration revealed that the ID of those stations correlates to other Stations but with a suffixed <code>444</code>.</p> <pre><code>WITH\nstations AS (\n  SELECT\n    station_start_id AS station_id,\n    station_start_name AS station_name,\n    date_start AS trip_date\n  FROM trips_raw\n  UNION ALL\n  SELECT\n    station_end_id AS station_id,\n    station_end_name AS station_name,\n    date_end AS trip_date\n  FROM trips_raw\n)\nSELECT\n  station_id,\n  station_name,\n  COUNT(*) AS record_count,\n  strftime(MIN(trip_date),'%Y-%m-%d') AS first_seen,\n  strftime(MAX(trip_date),'%Y-%m-%d') AS  last_seen,\nFROM stations\nWHERE station_id IN [\n  300100,\n  1020, 1020444,\n  1059, 1059444,\n  1190, 1190444,\n  3429, 3429444,\n  200032, 200032444,\n  200083, 200083444\n]\nGROUP BY station_id, station_name\nORDER BY station_name, first_seen;\n</code></pre> station_id station_name record_count first_seen last_seen 3429 Abbey Orchard Street, Westminster 15124 2024-01-01 2024-12-31 3429444 Abbey Orchard Street, Westminster_OLD 8288 2024-05-01 2024-08-15 1059 Albert Embankment, Vauxhall 12473 2024-01-01 2024-09-30 1059444 Albert Embankment, Vauxhall_OLD 3569 2024-10-01 2024-12-31 1190 Kennington Lane Rail Bridge, Vauxhall 32427 2024-01-01 2024-09-30 1190444 Kennington Lane Rail Bridge, Vauxhall_OLD 10531 2024-10-01 2024-12-31 200032 Kennington Oval, Oval 17686 2024-01-01 2024-12-31 200032444 Kennington Oval, Oval_OLD 356 2024-05-20 2024-06-02 1020 Leonard Circus , Shoreditch 20949 2024-01-01 2025-01-01 1020444 Leonard Circus , Shoreditch_OLD 11873 2024-05-01 2024-08-16 300100 Limburg Road, Clapham Junction 6803 2024-01-01 2024-12-31 300100 Limburg Road, Clapham Junction_OLD 1710 2024-10-01 2024-12-10 200083 Pritchard's Road, Bethnal Green 17595 2024-01-01 2024-09-30 200083444 Pritchard's Road, Bethnal Green_OLD 2462 2024-10-01 2024-12-31 <p>We now need to verify where the <code>_OLD</code> stations come from. To do so we add an additional dimension to the query, the original <code>filename</code> and use it to further divide the results.</p> <pre><code>WITH\nstations AS (\n  SELECT\n    station_start_id AS station_id,\n    station_start_name AS station_name,\n    date_start AS trip_date,\n    filename\n  FROM trips_raw\n  UNION ALL\n  SELECT\n    station_end_id AS station_id,\n    station_end_name AS station_name,\n    date_end AS trip_date,\n    filename\n  FROM trips_raw\n)\nSELECT\n  station_id,\n  station_name,\n  COUNT(*) AS record_count,\n  strftime(MIN(trip_date),'%Y-%m-%d') AS first_seen,\n  strftime(MAX(trip_date),'%Y-%m-%d') AS  last_seen,\n  filename,\nFROM stations\nWHERE station_id IN [3429, 3429444]\nGROUP BY station_id, station_name, filename\nORDER BY first_seen;\n</code></pre> <p>Here we present the results for a single station for brevity, but every other pair of stations shares the same behaviour:</p> <ul> <li>There is no overlap between the regular and <code>_OLD</code> stations.</li> <li>The <code>_OLD</code> suffix only appears during a time period, either mid-year or at the end of the year.</li> </ul> station_id station_name record_count first_seen last_seen filename 3429 Abbey Orchard Street, Westminster 500 2024-01-01 2024-01-14 data/format_0/387JourneyDataExtract01Jan2024-14Jan2024.csv.gz 3429 Abbey Orchard Street, Westminster 1099 2024-01-15 2024-01-31 data/format_0/388JourneyDataExtract15Jan2024-31Jan2024.csv.gz 3429 Abbey Orchard Street, Westminster 759 2024-02-01 2024-02-14 data/format_0/389JourneyDataExtract01Feb2024-14Feb2024.csv.gz 3429 Abbey Orchard Street, Westminster 785 2024-02-15 2024-02-29 data/format_0/390JourneyDataExtract15Feb2024-29Feb2024.csv.gz 3429 Abbey Orchard Street, Westminster 887 2024-03-01 2024-03-14 data/format_0/391JourneyDataExtract01Mar2024-14Mar2024.csv.gz 3429 Abbey Orchard Street, Westminster 917 2024-03-15 2024-03-31 data/format_0/392JourneyDataExtract15Mar2024-31Mar2024.csv.gz 3429 Abbey Orchard Street, Westminster 797 2024-04-01 2024-04-14 data/format_0/393JourneyDataExtract01Apr2024-14Apr2024.csv.gz 3429 Abbey Orchard Street, Westminster 1072 2024-04-15 2024-04-30 data/format_0/394JourneyDataExtract15Apr2024-30Apr2024.csv.gz 3429444 Abbey Orchard Street, Westminster_OLD 1011 2024-05-01 2024-05-14 data/format_0/395JourneyDataExtract01May2024-14May2024.csv.gz 3429444 Abbey Orchard Street, Westminster_OLD 1258 2024-05-15 2024-05-31 data/format_0/396JourneyDataExtract15May2024-31May2024.csv.gz 3429444 Abbey Orchard Street, Westminster_OLD 1020 2024-06-01 2024-06-14 data/format_0/397JourneyDataExtract01Jun2024-14Jun2024.csv.gz 3429444 Abbey Orchard Street, Westminster_OLD 1264 2024-06-15 2024-07-08 data/format_0/398JourneyDataExtract15Jun2024-30Jun2024.csv.gz 3429444 Abbey Orchard Street, Westminster_OLD 1208 2024-07-01 2024-07-14 data/format_0/399JourneyDataExtract01Jul2024-14Jul2024.csv.gz 3429444 Abbey Orchard Street, Westminster_OLD 1444 2024-07-15 2024-08-02 data/format_0/400JourneyDataExtract15Jul2024-31Jul2024.csv.gz 3429444 Abbey Orchard Street, Westminster_OLD 1072 2024-08-01 2024-08-14 data/format_1/401JourneyDataExtract01Aug2024-14Aug2024.csv.gz 3429444 Abbey Orchard Street, Westminster_OLD 11 2024-08-15 2024-08-15 data/format_1/402JourneyDataExtract15Aug2024-26Aug2024.csv.gz 3429 Abbey Orchard Street, Westminster 204 2024-08-21 2024-08-26 data/format_1/402JourneyDataExtract15Aug2024-26Aug2024.csv.gz 3429 Abbey Orchard Street, Westminster 1 2024-08-22 2024-08-22 data/format_1/401JourneyDataExtract01Aug2024-14Aug2024.csv.gz 3429 Abbey Orchard Street, Westminster 1633 2024-08-27 2024-09-17 data/format_1/403JourneyDataExtract27Aug2024-17Sep2024.csv.gz 3429 Abbey Orchard Street, Westminster 833 2024-09-18 2024-09-30 data/format_1/404JourneyDataExtract18Sep2024-30Sep2024.csv.gz 3429 Abbey Orchard Street, Westminster 999 2024-10-01 2024-11-21 data/format_0/405JourneyDataExtract01Oct2024-14Oct2024.csv.gz 3429 Abbey Orchard Street, Westminster 1411 2024-10-15 2024-10-31 data/format_0/406JourneyDataExtract15Oct2024-31Oct2024.csv.gz 3429 Abbey Orchard Street, Westminster 1062 2024-11-01 2024-11-14 data/format_0/407JourneyDataExtract01Nov2024-14Nov2024.csv.gz 3429 Abbey Orchard Street, Westminster 778 2024-11-15 2024-12-02 data/format_0/408JourneyDataExtract15Nov2024-30Nov2024.csv.gz 3429 Abbey Orchard Street, Westminster 792 2024-12-01 2024-12-14 data/format_0/409JourneyDataExtract01Dec2024-14Dec2024.csv.gz 3429 Abbey Orchard Street, Westminster 595 2024-12-15 2024-12-31 data/format_0/410JourneyDataExtract15Dec2024-31Dec2024.csv.gz <p>Since we found no documentation on the TfL website addressing this issue, we can only speculate about the reason for the change. We assume that during these time windows, the stations were temporarily relocated due to factors such as street remodeling or major construction work.</p> <p>Because we lack additional information about this decision, and given that there are many _OLD station records while the regular stations are missing data for several months, we decided to consolidate all _OLD station records into the corresponding regular stations. This allows us to maintain a continuous usage history for each station.</p> <p>For example, <code>3429444 | Abbey Orchard Street, Westminster_OLD</code> will become <code>3429 | Abbey Orchard Street, Westminster</code>.</p>"},{"location":"explore/stations/#station-format","title":"Station format","text":"<p>Station Names seem to follow the pattern <code>location, area</code>, both of which could become a calculated field later. We verify that all stations follow this pattern using regex.</p> <pre><code>WITH\nstations AS (\n  SELECT\n    station_start_id AS station_id,\n    station_start_name AS station_name\n  FROM trips_raw\n  UNION\n  SELECT\n    station_end_id AS station_id,\n    station_end_name AS station_name\n  FROM trips_raw\n)\nSELECT\nstation_id,\nstation_name\nFROM stations\nWHERE NOT regexp_matches(station_name, '^[^,]+\\s*,\\s*[^,]+$')\nGROUP BY station_id, station_name\nORDER BY station_id;\n</code></pre> station_id station_name 10626 Mechanical Workshop Penton 22168 Mechanical Workshop Clapham 200175 Wandsworth Rd, Isley Court, Wandsworth Road 300245 Clapham Road, Lingham Street, Stockwell <p>We found that 4 stations don't follow the pattern:</p> <ul> <li>Both Mechanical Workshop stations don't have a comma and are most likely used as internal locations.</li> <li>The other two stations have 2 commas, so we must be careful when splitting the <code>station_name</code> in the future.</li> </ul>"},{"location":"explore/stations/#workshop-stations","title":"Workshop Stations","text":"<p>We check the trips starting and ending on the two workshop stations found previously, to decide if these area real stations or internal ones.</p> <pre><code>SELECT COUNT() AS trip_count\nFROM trips_raw\nWHERE station_start_id IN [10626, 22168];\n</code></pre> trip_count 4 <pre><code>SELECT COUNT() AS trip_count\nFROM trips_raw\nWHERE station_end_id IN [10626, 22168];\n</code></pre> trip_count 428 <p>Our suspicion is confirmed by the large imbalance between trips starting and ending on both stations.</p> <p>Since these stations aren't part of the bike network they should be dropped on the cleaning phase.</p>"},{"location":"explore/trip_durations/","title":"Trip Duration","text":""},{"location":"explore/trip_durations/#distribution","title":"Distribution","text":"<p>We check the range and distribution of the time duration. We use minutes to get more meaningful numbers.</p> <pre><code>SELECT\n    ROUND(MIN(dm), 2) AS min,\n    ROUND(MAX(dm), 2) AS max,\n    ROUND(AVG(dm), 2) AS avg,\n    ROUND(quantile_disc(dm, 0.25), 2) AS q25,\n    ROUND(quantile_disc(dm, 0.50), 2) AS q50,\n    ROUND(quantile_disc(dm, 0.75), 2) AS q75\nFROM (\n    SELECT duration_ms/(1000 * 60) AS dm\n    FROM trips_raw\n);\n</code></pre> min max avg q25 q50 q75 0.01 116181.2 23.02 7.64 13.06 20.92 <p>We see that the distribution is heavily skewed towards the left. With most trips taking less than 21 minutes, but the longest ones taking multiple days.</p> <p>The minimum value is close to 0, our hypothesis being that those records are people:</p> <ul> <li>testing or demo-ing how to use the system.</li> <li>putting it back right after picking it up, either by mistake or because they changed their minds.</li> </ul> <p>The maximum value of 116181 minutes correspond to a total of about 80 days. We assume this and many other records correspond to people taking the bikes home, or the bike return not being registered in the stations.</p> <p>In any case, we will have to take into account the duration distribution when cleaning and extending the data, by either dropping certain the values or classifying them according to their duration.</p>"},{"location":"prepare/","title":"Prepare","text":"<p>In this section we cover all steps from data gathering to getting the data ready for exploration.</p>"},{"location":"prepare/#contents","title":"Contents","text":"<ul> <li>Gather: Data download and storage format.</li> <li>Describe: Overview of dataset schema.</li> <li>Model: Define entities and relationships.</li> <li>Load: Load CSV files into a single database.</li> </ul>"},{"location":"prepare/describe/","title":"Describe","text":""},{"location":"prepare/describe/#schema","title":"Schema","text":"<p>Every record in the dataset has the following fields:</p> Field Data Type Role Description Number UINT PK Record ID Start date TIMESTAMP Attribute Start of trip timestamp Start station number UINT FK Start station ID Start station STRING Attribute Name of start station End date TIMESTAMP Attribute End of trip timestamp End station number UINT FK End station ID End station STRING Attribute Name of end station Bike number UINT FK Bike ID Bike model STRING Attribute Type of bike Total duration STRING Attribute Human-readable representation of Total duration (ms) Total duration (ms) UINT Attribute Bike trip length in milliseconds"},{"location":"prepare/gather/","title":"Gather","text":"<p>The data source is the Transport for London (TfL) Open Data, in particular, their cycling section.</p> <p>We will focus on the data presented in the <code>usage-stats/</code> section.</p>"},{"location":"prepare/gather/#available-data","title":"Available data","text":"<p>The data is published as a single CSV file every two weeks. Individual links for the CSV files ranging from January 2016 up to May 2025 are available at the time of writting. There are also individual ZIP files with all the year CSV files for 2012 to 2016.</p> <p>We will focus our work on the data from 2024, since it is the most recent year with it's complete data available.</p>"},{"location":"prepare/gather/#download","title":"Download","text":"<p>Opening the TfL cycling repository and executing the following script on the browser console will return an array with the desired 24 elements. After that, one could use many methods to actually download the CSV files.</p> <pre><code>links = $$(\"a\");\nlinks.filter((e) =&gt; {\n    e.innerText.includes(\"2024.csv\");\n});\n</code></pre>"},{"location":"prepare/gather/#storage","title":"Storage","text":"<p>For long time storage we compress the files using Gzip, which reduces the dataset total disk space from 1.4 Gb to 300 Mb.</p> <pre><code># Plain bash\nfor file in data/*.csv;\ndo gzip $file;\ndone\n\n# leveraging GNU parallel\nls data/*.csv | parallel -j 6 gzip {}\n</code></pre>"},{"location":"prepare/load/","title":"Load","text":"<p>To allow a more flexible data exploration we must first join the records from the individual CSV files into a single database.</p>"},{"location":"prepare/load/#attribute-renaming","title":"Attribute renaming","text":"<p>To simplify the SQL query writing we assign new names for the attributes is assigned at load time to simplify SQL query writing.</p> Original Attribute Name New Attribute Name DuckDB type Number trip_id BIGINT Start date date_start TIMESTAMP Start station number station_start_id BIGINT Start station station_start_name VARCHAR End date date_end TIMESTAMP End station number station_end_id BIGINT End station station_end_name VARCHAR Bike number bike_id BIGINT Bike model bike_model VARCHAR Total duration duration_text VARCHAR Total duration (ms) duration_ms BIGINT <p>The SQL command to create the table:</p> <pre><code>CREATE TABLE trips_raw (\n    trip_id BIGINT,\n    date_start TIMESTAMP,\n    date_end TIMESTAMP,\n    station_start_id BIGINT,\n    station_end_id BIGINT,\n    station_start_name VARCHAR,\n    station_end_name VARCHAR,\n    bike_id BIGINT,\n    bike_model VARCHAR,\n    duration_text VARCHAR,\n    duration_ms BIGINT,\n    filename VARCHAR\n);\n</code></pre>"},{"location":"prepare/load/#csv-attribute-formatting","title":"CSV attribute formatting","text":"<p>After inspecting all the CSV files, we found that there are 2 types of formatting. The 4 files from August and September being the only odd ones.</p> <p>The main format:</p> <ul> <li>quotes on every field</li> <li>0-padded strings as IDs for Trips, Stations and Bikes</li> <li>Timestamp format is <code>YYYY-MM-DD HH:MM</code></li> </ul> \"Number\" \"Start date\" \"Start station number\" \"Start station\" \"End date\" \"End station number\" \"End station\" \"Bike number\" \"Bike model\" \"Total duration\" \"Total duration (ms)\" \"136666627\" \"2024-01-14 23:59\" \"001108\" \"North Wharf Road, Paddington\" \"2024-01-15 00:06\" \"003423\" \"Maida Vale, Maida Vale\" \"53020\" \"CLASSIC\" \"6m 47s\" \"407799\" \"136666625\" \"2024-01-14 23:57\" \"003447\" \"Gloucester Road (North), Kensington\" \"2024-01-15 00:05\" \"001214\" \"Kensington Olympia Station, Olympia\" \"54559\" \"CLASSIC\" \"8m 1s\" \"481276\" <p>The secondary format:</p> <ul> <li>quotes only on station name fields</li> <li>non-0-padded integers as IDs for Trips, Stations and Bikes</li> <li>Timestamp format is <code>DD/MM/YYYY HH:MM</code></li> </ul> Number Start date Start station number Start station End date End station number End station Bike number Bike model Total duration Total duration (ms) 142043054 14/08/2024 23:59 22165 \"Fisherman's Walk West,Canary Wharf\" 15/08/2024 00:40 200233 \"South Quay East, Canary Wharf\" 59663 CLASSIC 40m 53s 2453526 142043055 14/08/2024 23:59 22159 \"Ebury Bridge, Pimlico\" 15/08/2024 00:04 965 \"Tachbrook Street, Victoria\" 57811 CLASSIC 4m 54s 294201"},{"location":"prepare/load/#data-load","title":"Data Load","text":"<p>Now we write a DuckDB query that will read evert CSV file, apply the new naming scheme, parse the conflicting timestamp formats and append the records to our <code>trips_raw</code> table.</p> <p>Here is the query to import the secondary format files. The query from the main format remains differs only in the path and <code>timestampformat</code> flag.</p> <pre><code>-- Load format_1 files\nINSERT INTO trips_raw\n    SELECT\n        \"Number\" AS trip_id,\n        \"Start date\" AS date_start,\n        \"End date\" AS date_end,\n        \"Start station number\" AS station_start_id,\n        \"End station number\" AS station_end_id,\n        \"Start station\" AS station_start_name,\n        \"End station\" AS station_end_name,\n        \"Bike number\" AS bike_id,\n        \"Bike model\" AS bike_model,\n        \"Total duration\" AS duration_text,\n        \"Total duration (ms)\" AS duration_ms,\n        filename\n    FROM read_csv(\n        'data/format_1/*.csv.gz',\n        types={\n            'Start date': TIMESTAMP,\n            'Start station number': BIGINT,\n            'End station number': BIGINT,\n            'Number': BIGINT,\n            'Bike number': BIGINT,\n        },\n        filename=true,\n        timestampformat='%d/%m/%Y %H:%M'\n    );\n</code></pre>"},{"location":"prepare/model/","title":"Model","text":"<p>Applying data modeling and database normalization will provide a more manageable database and some principles to make it internally coherent.</p>"},{"location":"prepare/model/#entities","title":"Entities","text":"<p>A quick inspection of the original record attributes makes it obvious that it's merging attributes from 3 entities into individual records. We can easily normalize our <code>trips_raw</code> table into <code>trips</code>, <code>stations</code>and <code>bikes</code>.</p> Attribute Entity trip_id Trip date_start Trip station_start_id Station station_start_name Station date_end Trip station_end_id Station station_end_name Station bike_id Bike bike_model Bike duration_text Trip duration_ms Trip filename Trip <p>Resulting in the following Entity Relationship diagram:</p> <p></p>"},{"location":"prepare/model/#date-and-time-dimensions","title":"Date and Time dimensions","text":"<p>We could build a Date and even a Time dimension table to allow better slicing of the dataset. Here an example with the Bike and Stations table omitted for brevity.</p> <p></p> <p>To avoid premature normalization we won't perform this step until we see a clear need for it. We will use Date and Time functions on the queries as the need arise.</p>"},{"location":"process/","title":"Process","text":"<p>In this section we will compare the dataset against our data model, fix any inconsistencies and extend it with calculated fields.</p>"},{"location":"process/#contents","title":"Contents","text":"<ul> <li>Validate</li> <li>Clean</li> <li>Normalize</li> <li>Extend</li> </ul>"},{"location":"process/extend/","title":"Extend","text":"<p>To facilitate future analysis, new attributes will be added to the dataset.</p>"},{"location":"process/extend/#route-id","title":"Route ID","text":"<p>To study route data we can create an ID for the route by concatenating the <code>station_start_id</code> and <code>station_end_id</code> for each trip.</p> <pre><code>-- Unidirectional. A-to-B is different from B-to-A\nALTER TABLE trips\nADD COLUMN route_id_directional VARCHAR;\n\nUPDATE trips\nSET route_id_directional = CONCAT(station_start_id, '_', station_end_id);\n\n-- Direction agnostic. A-to-B is the same as B-to-A\nALTER TABLE trips\nADD COLUMN route_id_bidirectional VARCHAR;\n\nUPDATE trips\nSET route_id_bidirectional = CONCAT(\n    LEAST(station_start_id,station_end_id),\n    '_',\n    GREATEST(station_start_id,station_end_id)\n);\n</code></pre>"},{"location":"process/extend/#trip-duration-in-minutes","title":"Trip duration in minutes","text":"<p>From the Explore phase we now that 75% of the trips last less than 21 minutes, with a median of 13 minutes. With this reference we choose minutes as a more representative time measure.</p> <pre><code>-- Add trip duration in minutes column\nALTER TABLE trips\nADD COLUMN duration_minutes DOUBLE;\n\nUPDATE trips\nSET duration_minutes = duration_ms/(1000 * 60);\n</code></pre>"},{"location":"process/extend/#round-trip-flag","title":"Round trip flag","text":"<p>Further inspection of the <code>trips</code> table shows that we have both one-way trips and round trips.</p> <ul> <li>One-way trip: Starts and ends on different stations.</li> <li>Round trip: Starts and ends on different stations</li> </ul> <p>We add a boolean column to quickly filter between round and one-way trips.</p> <pre><code>-- Add round trip boolean column\nALTER TABLE trips\nADD COLUMN round_trip BOOL;\n\nUPDATE trips\nSET round_trip = (station_start_id == station_end_id);\n</code></pre>"},{"location":"process/normalize/","title":"Normalize","text":"<p>Having decided on a data model and checked the reference integrity of <code>bike_id</code> and <code>station_id</code> we can execute the normalization of the <code>trips_raw</code> table.</p> <p>We start by extracting the tables that don't have foreign keys. Most of the logic as described in the Data Validation section.</p>"},{"location":"process/normalize/#bikes","title":"Bikes","text":"<p>We take all unique <code>bike_id</code> from <code>trips_raw</code> and choose the last recorded <code>bike_model</code>.</p> <pre><code>CREATE TABLE bikes (\n    bike_id UBIGINT PRIMARY KEY,\n    model TEXT\n);\n\nINSERT INTO bikes (bike_id, model)\nWITH bikes_ranked AS (\n    SELECT\n        bike_id,\n        bike_model,\n        ROW_NUMBER() OVER (\n            PARTITION BY bike_id\n            ORDER BY date_end DESC\n        ) AS rn\n    FROM trips_raw\n)\nSELECT\n    bike_id,\n    bike_model\nFROM bikes_ranked\nWHERE rn = 1;\n\nCREATE INDEX bike_id_idx ON bikes (bike_id);\n</code></pre>"},{"location":"process/normalize/#stations","title":"Stations","text":"<p>We take all unique <code>station_id</code> from <code>trips_raw</code> and choose the last recorded <code>station_name</code>.</p> <pre><code>CREATE TABLE stations (\n    station_id UBIGINT PRIMARY KEY,\n    station_name TEXT\n);\n\nINSERT INTO stations (station_id, station_name)\nWITH stations_complete AS (\n    SELECT\n        station_start_id AS station_id,\n        station_start_name AS station_name,\n        date_start AS trip_date\n    FROM trips_raw\n    UNION ALL\n    SELECT\n        station_end_id AS station_id,\n        station_end_name AS station_name,\n        date_end AS trip_date\n    FROM trips_raw\n), stations_ranked AS (\n    SELECT\n        station_id,\n        station_name,\n        ROW_NUMBER() OVER (\n            PARTITION BY station_id\n            ORDER BY trip_date DESC\n    ) AS rn\n    FROM stations_complete\n)\nSELECT\n    station_id,\n    station_name\nFROM stations_ranked\nWHERE rn = 1;\n\nCREATE INDEX station_id_idx ON stations (station_id);\n</code></pre>"},{"location":"process/normalize/#trips","title":"Trips","text":"<p>For the trips table, simply selecting all attributes except for both <code>station_*_name</code> and <code>bike_model</code> will suffice. We will also drop the <code>duration_text</code> attribute since it does not provide analysis utility and can easily be regenerated on demand.</p> <pre><code>CREATE TABLE trips (\n    trip_id UBIGINT PRIMARY KEY,\n    date_start DATETIME,\n    date_end DATETIME,\n    bike_id UBIGINT,\n    station_start_id UBIGINT,\n    station_end_id UBIGINT,\n    duration_ms UBIGINT,\n    FOREIGN KEY (bike_id) REFERENCES bikes(bike_id),\n    FOREIGN KEY (station_start_id) REFERENCES stations(station_id),\n    FOREIGN KEY (station_end_id) REFERENCES stations(station_id),\n);\n\n\nINSERT INTO trips\nSELECT\n    trip_id,\n    date_start,\n    date_end,\n    bike_id,\n    station_start_id,\n    station_end_id,\n    duration_ms\nFROM trips_raw;\n\nCREATE INDEX trip_id_idx ON trips (trip_id);\n</code></pre>"},{"location":"process/validate/","title":"Validate","text":"<p>Before applying the final normalization step and dividing the single table into the relational model, we can verify the data coherence.</p>"},{"location":"process/validate/#bikes","title":"Bikes","text":"<p>We check that for every <code>bike_id</code> there are only 1 associated <code>bike_model</code>.</p> <pre><code>SELECT\n    bike_id,\n    COUNT(DISTINCT bike_model) AS model_variants\nFROM trips_raw\nGROUP BY bike_id\nHAVING model_variants &gt; 1\nORDER BY bike_id;\n</code></pre> bike_id model_variants 62308 2 62309 2 62524 2 62598 2 62732 2 62737 2 <p>Since there are multiple <code>bike_id</code> with more than 1 <code>bike_model</code> we need to decide on how to consolidate the inconsistency. For every pair of <code>bike_id</code> + <code>bike_model</code> we check the count of records and the first and last dates where that combination was recorded.</p> <pre><code>WITH target_ids AS (\n    SELECT bike_id\n    FROM trips_raw\n    GROUP BY bike_id\n    HAVING COUNT(DISTINCT bike_model) &gt; 1\n)\nSELECT\n    bike_id,\n    bike_model,\n    COUNT() AS record_count,\n    strftime(MIN(date_start), '%Y-%m-%d') AS first_seen,\n    strftime(MAX(date_end), '%Y-%m-%d') AS last_seen,\nFROM trips_raw\nWHERE bike_id IN (SELECT bike_id FROM target_ids)\nGROUP BY bike_id, bike_model\nORDER BY bike_id, first_seen;\n</code></pre> bike_id bike_model record_count first_seen last_seen 62308 CLASSIC 272 2024-07-05 2024-12-25 62308 PBSC_EBIKE 20 2024-08-06 2024-08-14 62309 CLASSIC 134 2024-07-04 2024-09-18 62309 PBSC_EBIKE 11 2024-08-01 2024-08-06 62524 CLASSIC 66 2024-07-11 2024-09-30 62524 PBSC_EBIKE 77 2024-10-01 2024-12-18 62598 CLASSIC 209 2024-07-30 2024-12-26 62598 PBSC_EBIKE 21 2024-08-01 2024-08-13 62732 CLASSIC 249 2024-07-03 2024-12-30 62732 PBSC_EBIKE 24 2024-08-01 2024-08-14 62737 CLASSIC 264 2024-07-04 2024-12-31 62737 PBSC_EBIKE 22 2024-08-01 2024-08-07 <p>We find that for most <code>bike_id</code> (except for 62524), the inconsistent records only were registered on the first weeks August of 2024. We will consider that all those records were caused by data input issues, were the incorrect <code>bike_model</code> was assigned to a <code>bike_id</code>and then reverted back.</p> <p>The only <code>bike_id</code> that don't follow the described pattern is <code>bike_id</code> 62524. This <code>bike_id</code> appeared for the first time on July as a <code>CLASSIC</code> and on October it was changed to a <code>PBSC_EBIKE</code> until December. We assume that, due to the lack of the back and forth seen in the other <code>bike_id</code>, the bike was incorrectly registered as a <code>CLASSIC</code> and corrected later.</p> <p>To fix both of these cases we can assign the last seen bike_model to each <code>bike_id</code>.</p> <pre><code>WITH bikes_ranked AS (\n    SELECT\n        bike_id,\n        bike_model,\n        ROW_NUMBER() OVER (\n            PARTITION BY bike_id\n            ORDER BY date_end DESC\n        ) AS rn\n    FROM trips_raw\n),\ntarget_ids AS (\n    SELECT bike_id\n    FROM trips_raw\n    GROUP BY bike_id\n    HAVING COUNT(DISTINCT bike_model) &gt; 1\n)\nSELECT\n    bike_id,\n    bike_model\nFROM bikes_ranked\nWHERE rn = 1\nAND bike_id IN (SELECT bike_id FROM target_ids)\nORDER BY bike_id;\n</code></pre> bike_id bike_model 62308 CLASSIC 62309 CLASSIC 62524 PBSC_EBIKE 62598 CLASSIC 62732 CLASSIC 62737 CLASSIC"},{"location":"process/validate/#stations","title":"Stations","text":"<p>In a similar fashion, we can check that every <code>station_id</code>has a unique <code>station_name</code>, with the additional care to merge all <code>start_station</code> and <code>end_station</code> attributes into a single stations table.</p> <pre><code>WITH stations AS (\n    SELECT\n        station_start_id AS station_id,\n        station_start_name AS station_name,\n    FROM trips_raw\n    UNION\n    SELECT\n        station_end_id AS station_id,\n        station_end_name AS station_name,\n    FROM trips_raw\n)\nSELECT\n    station_id,\n    COUNT(DISTINCT station_name) AS name_variants\nFROM stations\nGROUP BY station_id\nHAVING name_variants &gt; 1\nORDER BY station_id;\n</code></pre> station_id name_variants 964 2 1006 2 200118 2 300100 2 <pre><code>WITH\nstations AS (\n  SELECT\n    station_start_id AS station_id,\n    station_start_name AS station_name,\n    date_start AS trip_date\n  FROM trips_raw\n  UNION ALL\n  SELECT\n    station_end_id AS station_id,\n    station_end_name AS station_name,\n    date_end AS trip_date\n  FROM trips_raw\n)\nSELECT\n  station_id,\n  station_name,\n  COUNT(*) AS record_count,\n  strftime(MIN(trip_date),'%Y-%m-%d') AS first_seen,\n  strftime(MAX(trip_date),'%Y-%m-%d') AS  last_seen,\nFROM stations\nWHERE station_id IN (\n    SELECT station_id\n    FROM stations\n    GROUP BY station_id\n    HAVING COUNT(DISTINCT station_name) &gt; 1\n)\nGROUP BY station_id, station_name\nORDER BY station_id, first_date;\n</code></pre> station_id station_name record_count first_seen last_seen 964 Bath Street, St. Luke's 25057 2024-01-01 2024-12-31 964 Bath Street, St. Lke's 12952 2024-05-01 2024-08-26 1006 Bayley Street , Bloomsbury 21419 2024-01-01 2024-09-30 1006 Percy Street, Bloomsbury 6506 2024-10-01 2024-12-31 200118 Parkway, Camden Town 9730 2024-01-01 2024-09-23 200118 Albert Street, Camden Town 662 2024-11-19 2025-01-01 300100 Limburg Road, Clapham Junction 6629 2024-01-01 2024-12-31 300100 Limburg Road, Clapham Junction_OLD 1674 2024-10-01 2024-12-10 <p>With that information we can conclude that the reason for the name conflicts are:</p> <ul> <li>Station 964: Input error between May and August of 2024.</li> <li>Station 1006: Name change on October of 2024.</li> <li>Station 200118: Name change between October and November of 2024.<ul> <li>Possible closure during October, since records with that ID only started to reappear mid-November.</li> </ul> </li> <li>Station 300100: Input error between October and December of 2024.<ul> <li>We expected the <code>_OLD</code> suffix to appear from January to some point mid-year, but alas it didn't.</li> </ul> </li> </ul> <p>Since the TfL decided to keep the <code>station_id</code> after this changes, we will assume that:</p> <ul> <li><code>station_id</code> is a reliable identifier for each station.</li> <li>The name discrepancies don't represent an issue on the reference integrity.</li> <li>Until we extend the dataset with station location (latitude, longitude) data or similar, we can't but assume that the stations did not change it's geographical location during this period.</li> <li>The last seen name will be used when a Station Name is required</li> </ul> <p>Again, we write a query to build a normalized station table.</p> <pre><code>EXPLAIN\nWITH stations_unique AS (\n  SELECT\n    station_start_id AS station_id,\n    station_start_name AS station_name,\n    date_start AS trip_date\n  FROM trips_raw\n  UNION ALL\n  SELECT\n    station_end_id AS station_id,\n    station_end_name AS station_name,\n    date_end AS trip_date\n  FROM trips_raw\n), stations_complete AS (\n    SELECT\n        station_start_id AS station_id,\n        station_start_name AS station_name,\n        date_start AS trip_date\n    FROM trips_raw\n    UNION ALL\n    SELECT\n        station_end_id AS station_id,\n        station_end_name AS station_name,\n        date_end AS trip_date\n    FROM trips_raw\n), latest_station AS (\n    SELECT\n        station_id,\n        station_name,\n        ROW_NUMBER() OVER (\n            PARTITION BY station_id\n            ORDER BY trip_date DESC\n    ) AS rn\n    FROM stations_complete\n)\nSELECT\n    station_id,\n    station_name\nFROM latest_station\nWHERE rn = 1\nAND station_id IN (\n    SELECT station_id\n    FROM stations_unique\n    GROUP BY station_id\n    HAVING COUNT(DISTINCT station_name) &gt; 1\n)\nORDER BY station_id;\n</code></pre> station_id station_name 964 Bath Street, St. Luke's 1006 Percy Street, Bloomsbury 200118 Albert Street, Camden Town 300100 Limburg Road, Clapham Junction"}]}