{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Transport for London Cycling Data","text":"<p>Exploratory Data Analysis of the Transport for London (TfL) Cycling trip data.</p>"},{"location":"#overview","title":"Overview","text":"<p>The main goal is to apply the knowledge acquired during the Google Data Analytics Professional Certificate course, while keeping it simple enough to fit in the plaintext format of a github repository.</p> <p>I will analyze a single year worth of data, to limit the scope of the analysis.</p> <p>I will work with the data from 2024 since is the most recent complete year at the time of writing.</p>"},{"location":"#tools","title":"Tools","text":"<p>I will be working mainly with:</p> Tool Use DuckDB Parse, clean and query the dataset Mermaid General purpose diagrams Seaborn Complex plots"},{"location":"analysis/","title":"Analysis","text":"<p>Now we will make a review of the main entities of the dataset:</p> <ul> <li>Trips</li> <li>Bikes</li> <li>Stations</li> <li>Routes</li> </ul> <p>We will use simple statistical visualizations to gain knowledge on the attributes value ranges, distributions, correlations and general quirks before diving into modeling the relationships with more complex analysis tools.</p>"},{"location":"analysis/bikes/","title":"Bikes","text":"<p>A total of 14920 unique bikes were used during 2024.</p> <pre><code>SELECT COUNT(DISTINCT bike_id) FROM trips;\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 count(DISTINCT bike_id) \u2502\n\u2502          int64          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502          14920          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"analysis/bikes/#bike-trip-count","title":"Bike trip count","text":"<p>The trip count among all bikes shows a multi-mode distribution.</p> <p></p> <p>Possible causes:</p> <ul> <li>Location: Some bikes move between high-traffic stations.</li> <li>Age: New bikes could be added in batches mid-year.</li> </ul>"},{"location":"analysis/bikes/#bike-first-use-date","title":"Bike first use date","text":"<p>We get the first use date for each bike and plotting the cumulative proportion of bikes used as the year goes on.</p> <p></p> <p>We assume that a bike's first use is a good indicator of the bikes age inside the system. The bike could be new or was assigned a new ID after refurbishing.</p> <p>Here we can see that:</p> <ul> <li>most of the bikes were used in the first 2 months of the year.</li> <li>a steady amount of new bikes enters the float all-year round.</li> <li>a big increase in bikes is observed around July and October.</li> </ul>"},{"location":"analysis/bikes/#bike-age-vs-trip-count","title":"Bike age vs Trip count","text":"<p>Checking the correlation between a bike's first trip date and the total trip count we observe:</p> <ul> <li>Bike Age and Trip count have an inverse relationship.</li> <li>Older bikes have a wide distribution of trip counts.</li> <li>Both observed groups of new bikes (July and October) have similar trip counts and represent 2 of the modes in the trip count distribution.</li> </ul> <p></p>"},{"location":"analysis/trips/","title":"Trips","text":""},{"location":"analysis/trips/#trip-duration","title":"Trip duration","text":"<p>Trip duration times range from a few seconds to many days. With most them falling under around 120 minutes.</p> <p></p> <p></p>"},{"location":"analysis/trips/#round-trips","title":"Round-trips","text":"<p>Flagging trips with the same start and end station as round trips reveals some insights.</p> <p>Round trips are much less common than one-way trips.</p> <pre><code>SELECT\n  (station_end_id == station_start_id) AS round_trip,\n  COUNT(*) as count\nFROM trips_raw\nGROUP BY round_trip;\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 round_trip \u2502  count  \u2502\n\u2502  boolean   \u2502  int64  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 false      \u2502 8434689 \u2502\n\u2502 true       \u2502  320463 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p></p> <p>Closer inspection of round trips reveals a double mode.</p> <p></p> <p>Some possible causes:</p> <ul> <li>short trips:<ul> <li>users changing their mind about using the bike</li> <li>users testing the app functionality and/or the bikes themselves</li> <li>users taking short trips around the block</li> </ul> </li> <li>average trips:<ul> <li>users running errands</li> <li>taking longer pleasure routes</li> </ul> </li> </ul>"},{"location":"analysis/trips/#trips-under-1-minute","title":"Trips under 1 minute","text":"<p>There is an abundance of trips under 1 minute.</p> <p></p> <pre><code>SELECT\n  (station_end_id == station_start_id) AS round_trip,\n  COUNT(*) as count\nFROM trips_raw\nWHERE duration_ms / (1000 * 60) &lt; 1\nGROUP BY round_trip;\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 round_trip \u2502 count \u2502\n\u2502  boolean   \u2502 int64 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 false      \u2502  2148 \u2502\n\u2502 true       \u2502 67117 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Removing the round trips of less than 1 minute we get a more natural distribution.</p> <p></p> <p>Without more information it's impossible to discern the quality of the remaining 2 thousand trips records. Some possibilities:</p> <ul> <li>The two stations are really close and make it possible to get between them in less than a minute.</li> <li>Data quality issue, assigning the wrong ID to the end station.</li> </ul> <p>Going forward I will only focus on trips with durations:</p> <ul> <li>less or equal to 60 minutes</li> <li>different start and end locations if <code>duration_minutes</code> is less than 1 minute.</li> </ul> <pre><code>DELETE FROM trips\nWHERE\nduration_minutes &gt; 60\nOR\n(duration_minutes &lt; 1 AND station_start_id == station_end_id);\n</code></pre>"},{"location":"collection/","title":"Data Collection","text":"<p>Here we will describe the process to gather the dataset and make a general overview of it's structure.</p> <p>It is divided in the following steps:</p> <ul> <li>Gathering: download and merge files.</li> <li>Description: general overview of the dataset structure.</li> </ul>"},{"location":"collection/description/","title":"Data Description","text":""},{"location":"collection/description/#schema","title":"Schema","text":"<p>Every record in the dataset has the following fields:</p> Field Data Type Role Description Number UINT PK Record ID Start date TIMESTAMP Attribute Start of trip timestamp Start station number UINT FK Start station ID Start station STRING Attribute Name of start station End date TIMESTAMP Attribute End of trip timestamp End station number UINT FK End station ID End station STRING Attribute Name of end station Bike number UINT FK Bike ID Bike model STRING Attribute Type of bike Total duration STRING Attribute Human-readable representation of Total duration (ms) Total duration (ms) UINT Attribute Bike trip length in milliseconds"},{"location":"collection/gathering/","title":"Data Gathering","text":"<p>The data source is the Transport for London (TfL) Open Data, in particular, their cycling section.</p> <p>We will focus on the data presented in the <code>usage-stats/</code> section.</p>"},{"location":"collection/gathering/#available-data","title":"Available data","text":"<p>The data is published as a single CSV file every two weeks. Individual links for the CSV files ranging from January 2016 up to May 2025 are available at the time of writting. There are also individual ZIP files with all the year CSV files for 2012 to 2016.</p> <p>We will focus our work on the data from 2024, since it is the most recent year with it's complete data available.</p>"},{"location":"collection/gathering/#download","title":"Download","text":"<p>Opening the TfL cycling repository and executing the following script on the browser console will return an array with the desired 24 elements. After that, one could use many methods to actually download the CSV files.</p> <pre><code>links = $$(\"a\");\nlinks.filter((e) =&gt; {\n    e.innerText.includes(\"2024.csv\");\n});\n</code></pre>"},{"location":"collection/gathering/#storage","title":"Storage","text":"<p>For long time storage we compress the files using Gzip, which reduces the dataset total disk space from 1.4 Gb to 300 Mb.</p> <pre><code># Plain bash\nfor file in data/*.csv;\ndo gzip $file;\ndone\n\n# leveraging GNU parallel\nls data/*.csv | parallel -j 6 gzip {}\n</code></pre>"},{"location":"process/","title":"Process","text":"<p>In this section we will take a general overview of the dataset and it's entities, fix any inconsistencies and extend the original dataset with calculated fields.</p> <ul> <li>Gather:</li> <li>Load: Parse and merge all CSV files into a single database.</li> <li>Explore:</li> <li>Model: Transform the original schema to increase the analysis flexibility.</li> <li>Validate: Find and correct inconsistencies against our theory model.</li> <li>Clean:</li> <li>Normalize:</li> <li>Extend: Create new attributes to facilitate working with the dataset.</li> </ul>"},{"location":"process/extend/","title":"Data Extension","text":""},{"location":"process/load/","title":"Data Loading","text":"<p>To allow a more flexible data exploration we must first join the records from the individual CSV files into a single database.</p>"},{"location":"process/load/#attribute-renaming","title":"Attribute renaming","text":"<p>To simplify the SQL query writing we assign new names for the attributes is assigned at load time to simplify SQL query writing.</p> Original Attribute Name New Attribute Name DuckDB type Number trip_id BIGINT Start date date_start TIMESTAMP Start station number station_start_id BIGINT Start station station_start_name VARCHAR End date date_end TIMESTAMP End station number station_end_id BIGINT End station station_end_name VARCHAR Bike number bike_id BIGINT Bike model bike_model VARCHAR Total duration duration_text VARCHAR Total duration (ms) duration_ms BIGINT <p>The SQL command to create the table:</p> <pre><code>CREATE TABLE trips_raw (\n    trip_id BIGINT,\n    date_start TIMESTAMP,\n    date_end TIMESTAMP,\n    station_start_id BIGINT,\n    station_end_id BIGINT,\n    station_start_name VARCHAR,\n    station_end_name VARCHAR,\n    bike_id BIGINT,\n    bike_model VARCHAR,\n    duration_text VARCHAR,\n    duration_ms BIGINT,\n);\n</code></pre>"},{"location":"process/load/#csv-attribute-formatting","title":"CSV attribute formatting","text":"<p>After inspecting all the CSV files, we found that there are 2 types of formatting. The 4 files from August and September being the only odd ones.</p> <p>The main format:</p> <ul> <li>quotes on every field</li> <li>0-padded strings as IDs for Trips, Stations and Bikes</li> <li>Timestamp format is <code>YYYY-MM-DD HH:MM</code></li> </ul> \"Number\" \"Start date\" \"Start station number\" \"Start station\" \"End date\" \"End station number\" \"End station\" \"Bike number\" \"Bike model\" \"Total duration\" \"Total duration (ms)\" \"136666627\" \"2024-01-14 23:59\" \"001108\" \"North Wharf Road, Paddington\" \"2024-01-15 00:06\" \"003423\" \"Maida Vale, Maida Vale\" \"53020\" \"CLASSIC\" \"6m 47s\" \"407799\" \"136666625\" \"2024-01-14 23:57\" \"003447\" \"Gloucester Road (North), Kensington\" \"2024-01-15 00:05\" \"001214\" \"Kensington Olympia Station, Olympia\" \"54559\" \"CLASSIC\" \"8m 1s\" \"481276\" <p>The secondary format:</p> <ul> <li>quotes only on station name fields</li> <li>non-0-padded integers as IDs for Trips, Stations and Bikes</li> <li>Timestamp format is <code>DD/MM/YYYY HH:MM</code></li> </ul> Number Start date Start station number Start station End date End station number End station Bike number Bike model Total duration Total duration (ms) 142043054 14/08/2024 23:59 22165 \"Fisherman's Walk West,Canary Wharf\" 15/08/2024 00:40 200233 \"South Quay East, Canary Wharf\" 59663 CLASSIC 40m 53s 2453526 142043055 14/08/2024 23:59 22159 \"Ebury Bridge, Pimlico\" 15/08/2024 00:04 965 \"Tachbrook Street, Victoria\" 57811 CLASSIC 4m 54s 294201"},{"location":"process/load/#data-load","title":"Data Load","text":"<p>Now we write a DuckDB query that will read evert CSV file, apply the new naming scheme, parse the conflicting timestamp formats and append the records to our <code>trips_raw</code> table.</p> <p>Here is the query to import the secondary format files. The query from the main format remains differs only in the path and <code>timestampformat</code> flag.</p> <pre><code>-- Load format_1 files\nINSERT INTO trips_raw\n    SELECT\n        \"Number\" AS trip_id,\n        \"Start date\" AS date_start,\n        \"End date\" AS date_end,\n        \"Start station number\" AS station_start_id,\n        \"End station number\" AS station_end_id,\n        \"Start station\" AS station_start_name,\n        \"End station\" AS station_end_name,\n        \"Bike number\" AS bike_id,\n        \"Bike model\" AS bike_model,\n        \"Total duration\" AS duration_text,\n        \"Total duration (ms)\" AS duration_ms,\n    FROM read_csv(\n        'data/format_1/*.csv.gz',\n        types={\n            'Start date': TIMESTAMP,\n            'Start station number': BIGINT,\n            'End station number': BIGINT,\n            'Number': BIGINT,\n            'Bike number': BIGINT,\n        },\n        timestampformat='%d/%m/%Y %H:%M'\n    );\n</code></pre>"},{"location":"process/model/","title":"Data Modeling","text":"<p>Applying data modeling and database normalization will provide a more manageable database and some principles to make it internally coherent.</p>"},{"location":"process/model/#entities","title":"Entities","text":"<p>A quick inspection of the original record attributes makes it obvious that it's merging attributes from 3 entities into individual records. We can easily normalize our <code>trips_raw</code> table into <code>trips</code>, <code>stations</code>and <code>bikes</code>.</p> Attribute Entity trip_id Trip date_start Trip station_start_id Station station_start_name Station date_end Trip station_end_id Station station_end_name Station bike_id Bike bike_model Bike duration_text Trip duration_ms Trip <p>Resulting in the following Entity Relationship diagram:</p> <p></p>"},{"location":"process/model/#date-and-time-dimensions","title":"Date and Time dimensions","text":"<p>We could build a Date and even a Time dimension table to allow better slicing of the dataset. Here an example with the Bike and Stations table omitted for brevity.</p> <p></p> <p>To avoid premature normalization we won't perform this step until we see a clear need for it. We will use Date and Time functions on the queries as the need arise.</p>"},{"location":"process/normalize/","title":"Normalization","text":"<p>Having decided on a data model and checked the reference integrity of <code>bike_id</code> and <code>station_id</code> we can execute the normalization of the <code>trips_raw</code> table.</p> <p>We start by extracting the tables that don't have foreign keys. Most of the logic as described in the Data Validation section.</p>"},{"location":"process/normalize/#bikes","title":"Bikes","text":"<p>We take all unique <code>bike_id</code> from <code>trips_raw</code> and choose the last recorded <code>bike_model</code>.</p> <pre><code>CREATE TABLE bikes (\n    bike_id UBIGINT PRIMARY KEY,\n    model TEXT\n);\n\nINSERT INTO bikes (bike_id, model)\nWITH bikes_ranked AS (\n    SELECT\n        bike_id,\n        bike_model,\n        ROW_NUMBER() OVER (\n            PARTITION BY bike_id\n            ORDER BY date_end DESC\n        ) AS rn\n    FROM trips_raw\n)\nSELECT\n    bike_id,\n    bike_model\nFROM bikes_ranked\nWHERE rn = 1;\n\nCREATE INDEX bike_id_idx ON bikes (bike_id);\n</code></pre>"},{"location":"process/normalize/#stations","title":"Stations","text":"<p>We take all unique <code>station_id</code> from <code>trips_raw</code> and choose the last recorded <code>station_name</code>.</p> <pre><code>CREATE TABLE stations (\n    station_id UBIGINT PRIMARY KEY,\n    station_name TEXT\n);\n\nINSERT INTO stations (station_id, station_name)\nWITH stations_complete AS (\n    SELECT\n        station_start_id AS station_id,\n        station_start_name AS station_name,\n        date_start AS trip_date\n    FROM trips_raw\n    UNION ALL\n    SELECT\n        station_end_id AS station_id,\n        station_end_name AS station_name,\n        date_end AS trip_date\n    FROM trips_raw\n), stations_ranked AS (\n    SELECT\n        station_id,\n        station_name,\n        ROW_NUMBER() OVER (\n            PARTITION BY station_id\n            ORDER BY trip_date DESC\n    ) AS rn\n    FROM stations_complete\n)\nSELECT\n    station_id,\n    station_name\nFROM stations_ranked\nWHERE rn = 1;\n\nCREATE INDEX station_id_idx ON stations (station_id);\n</code></pre>"},{"location":"process/normalize/#trips","title":"Trips","text":"<p>For the trips table, simply selecting all attributes except for both <code>station_*_name</code> and <code>bike_model</code> will suffice. We will also drop the <code>duration_text</code> attribute since it does not provide analysis utility and can easily be regenerated on demand.</p> <pre><code>CREATE TABLE trips (\n    trip_id UBIGINT PRIMARY KEY,\n    date_start DATETIME,\n    date_end DATETIME,\n    bike_id UBIGINT,\n    station_start_id UBIGINT,\n    station_end_id UBIGINT,\n    duration_ms UBIGINT,\n    FOREIGN KEY (bike_id) REFERENCES bikes(bike_id),\n    FOREIGN KEY (station_start_id) REFERENCES stations(station_id),\n    FOREIGN KEY (station_end_id) REFERENCES stations(station_id),\n);\n\n\nINSERT INTO trips\nSELECT\n    trip_id,\n    date_start,\n    date_end,\n    bike_id,\n    station_start_id,\n    station_end_id,\n    duration_ms\nFROM trips_raw;\n\nCREATE INDEX trip_id_idx ON trips (trip_id);\n</code></pre>"},{"location":"process/validate/","title":"Data Validation","text":"<p>Before applying the final normalization step and dividing the single table into the relational model, we can verify the data coherence.</p>"},{"location":"process/validate/#bikes","title":"Bikes","text":"<p>We check that for every <code>bike_id</code> there are only 1 associated <code>bike_model</code>.</p> <pre><code>SELECT\n    bike_id,\n    COUNT(DISTINCT bike_model) AS model_variants\nFROM trips_raw\nGROUP BY bike_id\nHAVING model_variants &gt; 1\nORDER BY bike_id;\n</code></pre> bike_id model_variants 62308 2 62309 2 62524 2 62598 2 62732 2 62737 2 <p>Since there are multiple <code>bike_id</code> with more than 1 <code>bike_model</code> we need to decide on how to consolidate the inconsistency. For every pair of <code>bike_id</code> + <code>bike_model</code> we check the count of records and the first and last dates where that combination was recorded.</p> <pre><code>WITH target_ids AS (\n    SELECT bike_id\n    FROM trips_raw\n    GROUP BY bike_id\n    HAVING COUNT(DISTINCT bike_model) &gt; 1\n)\nSELECT\n    bike_id,\n    bike_model,\n    COUNT() AS record_count,\n    strftime(MIN(date_start), '%Y-%m-%d') AS first_seen,\n    strftime(MAX(date_end), '%Y-%m-%d') AS last_seen,\nFROM trips_raw\nWHERE bike_id IN (SELECT bike_id FROM target_ids)\nGROUP BY bike_id, bike_model\nORDER BY bike_id, first_seen;\n</code></pre> bike_id bike_model record_count first_seen last_seen 62308 CLASSIC 272 2024-07-05 2024-12-25 62308 PBSC_EBIKE 20 2024-08-06 2024-08-14 62309 CLASSIC 134 2024-07-04 2024-09-18 62309 PBSC_EBIKE 11 2024-08-01 2024-08-06 62524 CLASSIC 66 2024-07-11 2024-09-30 62524 PBSC_EBIKE 77 2024-10-01 2024-12-18 62598 CLASSIC 209 2024-07-30 2024-12-26 62598 PBSC_EBIKE 21 2024-08-01 2024-08-13 62732 CLASSIC 249 2024-07-03 2024-12-30 62732 PBSC_EBIKE 24 2024-08-01 2024-08-14 62737 CLASSIC 264 2024-07-04 2024-12-31 62737 PBSC_EBIKE 22 2024-08-01 2024-08-07 <p>We find that for most <code>bike_id</code> (except for 62524), the inconsistent records only were registered on the first weeks August of 2024. We will consider that all those records were caused by data input issues, were the incorrect <code>bike_model</code> was assigned to a <code>bike_id</code>and then reverted back.</p> <p>The only <code>bike_id</code> that don't follow the described pattern is <code>bike_id</code> 62524. This <code>bike_id</code> appeared for the first time on July as a <code>CLASSIC</code> and on October it was changed to a <code>PBSC_EBIKE</code> until December. We assume that, due to the lack of the back and forth seen in the other <code>bike_id</code>, the bike was incorrectly registered as a <code>CLASSIC</code> and corrected later.</p> <p>To fix both of these cases we can assign the last seen bike_model to each <code>bike_id</code>.</p> <pre><code>WITH bikes_ranked AS (\n    SELECT\n        bike_id,\n        bike_model,\n        ROW_NUMBER() OVER (\n            PARTITION BY bike_id\n            ORDER BY date_end DESC\n        ) AS rn\n    FROM trips_raw\n),\ntarget_ids AS (\n    SELECT bike_id\n    FROM trips_raw\n    GROUP BY bike_id\n    HAVING COUNT(DISTINCT bike_model) &gt; 1\n)\nSELECT\n    bike_id,\n    bike_model\nFROM bikes_ranked\nWHERE rn = 1\nAND bike_id IN (SELECT bike_id FROM target_ids)\nORDER BY bike_id;\n</code></pre> bike_id bike_model 62308 CLASSIC 62309 CLASSIC 62524 PBSC_EBIKE 62598 CLASSIC 62732 CLASSIC 62737 CLASSIC"},{"location":"process/validate/#stations","title":"Stations","text":"<p>In a similar fashion, we can check that every <code>station_id</code>has a unique <code>station_name</code>, with the additional care to merge all <code>start_station</code> and <code>end_station</code> attributes into a single stations table.</p> <pre><code>WITH stations AS (\n    SELECT\n        station_start_id AS station_id,\n        station_start_name AS station_name,\n    FROM trips_raw\n    UNION\n    SELECT\n        station_end_id AS station_id,\n        station_end_name AS station_name,\n    FROM trips_raw\n)\nSELECT\n    station_id,\n    COUNT(DISTINCT station_name) AS name_variants\nFROM stations\nGROUP BY station_id\nHAVING name_variants &gt; 1\nORDER BY station_id;\n</code></pre> station_id name_variants 964 2 1006 2 200118 2 300100 2 <pre><code>WITH\nstations AS (\n  SELECT\n    station_start_id AS station_id,\n    station_start_name AS station_name,\n    date_start AS trip_date\n  FROM trips_raw\n  UNION ALL\n  SELECT\n    station_end_id AS station_id,\n    station_end_name AS station_name,\n    date_end AS trip_date\n  FROM trips_raw\n)\nSELECT\n  station_id,\n  station_name,\n  COUNT(*) AS record_count,\n  strftime(MIN(trip_date),'%Y-%m-%d') AS first_seen,\n  strftime(MAX(trip_date),'%Y-%m-%d') AS  last_seen,\nFROM stations\nWHERE station_id IN (\n    SELECT station_id\n    FROM stations\n    GROUP BY station_id\n    HAVING COUNT(DISTINCT station_name) &gt; 1\n)\nGROUP BY station_id, station_name\nORDER BY station_id, first_date;\n</code></pre> station_id station_name record_count first_seen last_seen 964 Bath Street, St. Luke's 25057 2024-01-01 2024-12-31 964 Bath Street, St. Lke's 12952 2024-05-01 2024-08-26 1006 Bayley Street , Bloomsbury 21419 2024-01-01 2024-09-30 1006 Percy Street, Bloomsbury 6506 2024-10-01 2024-12-31 200118 Parkway, Camden Town 9730 2024-01-01 2024-09-23 200118 Albert Street, Camden Town 662 2024-11-19 2025-01-01 300100 Limburg Road, Clapham Junction 6629 2024-01-01 2024-12-31 300100 Limburg Road, Clapham Junction_OLD 1674 2024-10-01 2024-12-10 <p>With that information we can conclude that the reason for the name conflicts are:</p> <ul> <li>Station 964: Input error between May and August of 2024.</li> <li>Station 1006: Name change on October of 2024.</li> <li>Station 200118: Name change between October and November of 2024.<ul> <li>Possible closure during October, since records with that ID only started to reappear mid-November.</li> </ul> </li> <li>Station 300100: Input error between October and December of 2024.<ul> <li>We expected the <code>_OLD</code> suffix to appear from January to some point mid-year, but alas it didn't.</li> </ul> </li> </ul> <p>Since the TfL decided to keep the <code>station_id</code> after this changes, we will assume that:</p> <ul> <li><code>station_id</code> is a reliable identifier for each station.</li> <li>The name discrepancies don't represent an issue on the reference integrity.</li> <li>Until we extend the dataset with station location (latitude, longitude) data or similar, we can't but assume that the stations did not change it's geographical location during this period.</li> <li>The last seen name will be used when a Station Name is required</li> </ul> <p>Again, we write a query to build a normalized station table.</p> <pre><code>EXPLAIN\nWITH stations_unique AS (\n  SELECT\n    station_start_id AS station_id,\n    station_start_name AS station_name,\n    date_start AS trip_date\n  FROM trips_raw\n  UNION ALL\n  SELECT\n    station_end_id AS station_id,\n    station_end_name AS station_name,\n    date_end AS trip_date\n  FROM trips_raw\n), stations_complete AS (\n    SELECT\n        station_start_id AS station_id,\n        station_start_name AS station_name,\n        date_start AS trip_date\n    FROM trips_raw\n    UNION ALL\n    SELECT\n        station_end_id AS station_id,\n        station_end_name AS station_name,\n        date_end AS trip_date\n    FROM trips_raw\n), latest_station AS (\n    SELECT\n        station_id,\n        station_name,\n        ROW_NUMBER() OVER (\n            PARTITION BY station_id\n            ORDER BY trip_date DESC\n    ) AS rn\n    FROM stations_complete\n)\nSELECT\n    station_id,\n    station_name\nFROM latest_station\nWHERE rn = 1\nAND station_id IN (\n    SELECT station_id\n    FROM stations_unique\n    GROUP BY station_id\n    HAVING COUNT(DISTINCT station_name) &gt; 1\n)\nORDER BY station_id;\n</code></pre> station_id station_name 964 Bath Street, St. Luke's 1006 Percy Street, Bloomsbury 200118 Albert Street, Camden Town 300100 Limburg Road, Clapham Junction"}]}